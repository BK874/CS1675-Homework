
<!-- saved from url=(0057)https://people.cs.pitt.edu/~kovashka/cs1675_fa18/hw7.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CS1675: Homework 7</title>
</head>
<body>
<h2>CS1675: Homework 7</h2>
<b>Due:</b> 11/<font color="red">18</font>/2018, 11:59pm 
<br><br>

This assignment is worth 55 points. 
<br><br><br>

<u>Part I: Computing weight updates by hand</u> (15 points)
<br><br>
In recitation, we show how to compute activations in a neural network, and how to perform stochastic gradient descent to train it. We compute activations for two example networks, but only show how to train one of them. Show how to train the second network using just a single example, <font face="courier new">x = [1 1], y = [0 0]</font> (note that in this case, the label is a vector). Initialize all weights to 0.05. <font color="red">Use a learning rate of 0.3.</font> Include your answers in text form in a file <font face="courier new">report.pdf/docx</font>.
<br><br><br>

<u>Part II: Training a neural network</u> (25 points)
<br><br>
In this exercise, you will write code to train and evaluate a very simple neural network. We will follow the example in Bishop that uses a single hidden layer, a tanh function at the hidden layer and an identity function at the output layer, and a squared error loss. The network will have 30 hidden neurons (i.e. M=30) and 1 output neuron (i.e. K=1). To implement it, follow the equations in the slides and your textbook. 
<br><br><br>
First, write a <font face="courier new">function [y_pred, Z] = forward(X, W1, W2)</font> that computes activations from the front towards the back of the network, using fixed input features and weights. Also use the forward pass function to evaluate your network after training. 
<br><br>
<b>Inputs:</b> 
<ul>
<li>an NxD matrix <font face="courier new">X</font> of features, where N is the number of samples and D is the number of feature dimensions,</li>
<li>an MxD matrix <font face="courier new">W1</font> of weights between the first and second layer of the network, where M is the number of hidden neurons, and</li>
<li>an 1xM matrix <font face="courier new">W2</font> of weights between the second and third layer of the network, where there is a single neuron at the output layer</li>
</ul>
<b>Outputs:</b>
<ul>
<li>[5 pts] an Nx1 vector <font face="courier new">y_pred</font> containing the outputs at the last layer for all N samples, and</li>
<li>[5 pts] an NxM matrix <font face="courier new">Z</font> containing the activations for all M hidden neurons of all N samples.</li>
</ul>
Second, write a <font face="courier new">function [W1, W2, error_over_time] = backward(X, y, M, iters, eta)</font> that performs training using backpropagation (and calls the activation computation function as it iterates). Construct the network in this function, i.e. create the weight matrices and initialize the weights to small random numbers, then iterate: pick a training sample, compute the error at the output, then backpropagate to the hidden layer, and update the weights with the resulting error. 
<br><br>
<b>Inputs:</b> 
<ul>
<li>an NxD matrix <font face="courier new">X</font> of features, where N is the number of samples and D is the number of feature dimensions,</li>
<li>an Nx1 vector <font face="courier new">y</font> containing the ground-truth labels for the N samples,</li>
<li>a scalar <font face="courier new">M</font> containing the number
  of hidden <font color="red">neurons</font> to use, </li>
<li>a scalar <font face="courier new">iters</font> defining how many iterations to run (one sample used in each), and</li>
<li>a scalar <font face="courier new">eta</font> defining the learning rate to use. </li>
</ul>
<b>Outputs:</b>
<ul>
<li>[10 pts] <font face="courier new">W1</font> and <font face="courier new">W2</font>, defined as above for <font face="courier new">forward</font>, and</li>
<li>[5 pts] an <font face="courier new">iters</font>x1 vector <font face="courier new">error_over_time</font> that contains the error on the sample used in each iteration.
</li></ul>
<br>

<u>Part III: Testing your neural network on wine quality</u> (15 points)
<br><br>
We will use the Wine Quality dataset from HW3 to test the neural network implementation. Write your code in a script <font face="courier new">neural_net.m</font>.
<ol>
<li>[2 pts] Load the wine dataset, define the train/test split as in HW3, and set the number of hidden units, the number of iterations to run, and the learning rate. </li>
<li>[3 pts] Call the <font face="courier new">backward</font> function to construct and train the network. Use 1000 iterations and 30 hidden neurons. </li>
<li>[5 pts] Then call the <font face="courier new">forward</font> function to make predictions and compute the root mean squared error between predicted and ground-truth labels, <font face="courier new">sqrt(mean((y_test_pred - y_test).^2))</font>. Report this number in a file <font face="courier new">report.pdf/docx</font> 
</li><li>[5 pts] Experiment with three different values of the learning rate. For each, plot the error over time (output by <font face="courier new">backward</font> above). Include these plots in your report. My plot looks like this: <br><img src="./CS1675_ Homework 7_files/mse_plot.png" width="500/"></li>
</ol>
<br>

<b>Submission:</b> Please include the following files:
<ul>
<li><font face="courier new">report.pdf/docx</font></li>
<li><font face="courier new">forward.m</font></li>
<li><font face="courier new">backward.m</font></li>
<li><font face="courier new">neural_net.m</font></li>
</ul>

<br>


<div id="UMS_TOOLTIP" style="position: absolute; cursor: pointer; z-index: 2147483647; background: transparent; top: -100000px; left: -100000px;"></div></body><umsdataelement id="UMSSendDataEventElement"></umsdataelement></html>